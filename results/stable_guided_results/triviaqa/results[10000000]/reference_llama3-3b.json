{"triviaqa": {"alias": "triviaqa", "exact_match,remove_whitespace": 0.5895006687472135, "exact_match_stderr,remove_whitespace": 0.0036724073629997135}, "command": "Command: evaluate_task.py --model_name llama3-3b --checkpoint_path /models/llama3-3b/ --limit 10000000 --task_name triviaqa --output_path /workspace/results/siot_results/triviaqa/results[10000000]/reference.json"}
{"triviaqa": {"alias": "triviaqa", "exact_match,remove_whitespace": 0.3494761480160499, "exact_match_stderr,remove_whitespace": 0.0035595316101435554}, "command": "Command: evaluate_task.py --model_name opt-6.7b --checkpoint_path /local/huzaifa/workspace/models/opt-6.7b/ --limit 10000000 --task_name triviaqa --output_path /local/huzaifa/workspace/CoreInfer/stable_guided_results/triviaqa/results[10000000]/reference_opt-6.7b.json --token_sparsity 0.2 --sparsity 0.4 --method stable_guided --start_num 4 --end_num 26"}
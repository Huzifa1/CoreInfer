{"triviaqa": {"alias": "triviaqa", "exact_match,remove_whitespace": 0.08476370931787784, "exact_match_stderr,remove_whitespace": 0.002079333087361972}, "command": "Command: evaluate_task.py --model_name llama3-3b --checkpoint_path /models/llama3-3b/ --limit 10000000 --task_name triviaqa --output_path /workspace/results/siot_results/triviaqa/results[10000000]/coreinfer_default.json --method stable_guided --token_sparsity 0.2 --sparsity 0.4 --method stable_guided --start_num 3 --end_num 25"}